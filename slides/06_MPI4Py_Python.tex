%% This file was auto-generated by IPython, do NOT edit
%% Conversion from the original notebook file:
%% 06_MPI4Py_Python.ipynb
%%
\documentclass[11pt,english]{article}

%% This is the automatic preamble used by IPython.  Note that it does *not*
%% include a documentclass declaration, that is added at runtime to the overall
%% document.

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{ucs}
\usepackage[utf8x]{inputenc}

% needed for markdown enumerations to work
\usepackage{enumerate}

% Slightly bigger margins than the latex defaults
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=2.5cm,rmargin=2.5cm}

% Define a few colors for use in code, links and cell shading
\usepackage{color}
\definecolor{orange}{cmyk}{0,0.4,0.8,0.2}
\definecolor{darkorange}{rgb}{.71,0.21,0.01}
\definecolor{darkgreen}{rgb}{.12,.54,.11}
\definecolor{myteal}{rgb}{.26, .44, .56}
\definecolor{gray}{gray}{0.45}
\definecolor{lightgray}{gray}{.95}
\definecolor{mediumgray}{gray}{.8}
\definecolor{inputbackground}{rgb}{.95, .95, .85}
\definecolor{outputbackground}{rgb}{.95, .95, .95}
\definecolor{traceback}{rgb}{1, .95, .95}

% Framed environments for code cells (inputs, outputs, errors, ...).  The
% various uses of \unskip (or not) at the end were fine-tuned by hand, so don't
% randomly change them unless you're sure of the effect it will have.
\usepackage{framed}

% remove extraneous vertical space in boxes
\setlength\fboxsep{0pt}

% codecell is the whole input+output set of blocks that a Code cell can
% generate.

% TODO: unfortunately, it seems that using a framed codecell environment breaks
% the ability of the frames inside of it to be broken across pages.  This
% causes at least the problem of having lots of empty space at the bottom of
% pages as new frames are moved to the next page, and if a single frame is too
% long to fit on a page, will completely stop latex from compiling the
% document.  So unless we figure out a solution to this, we'll have to instead
% leave the codecell env. as empty.  I'm keeping the original codecell
% definition here (a thin vertical bar) for reference, in case we find a
% solution to the page break issue.

%% \newenvironment{codecell}{%
%%     \def\FrameCommand{\color{mediumgray} \vrule width 1pt \hspace{5pt}}%
%%    \MakeFramed{\vspace{-0.5em}}}
%%  {\unskip\endMakeFramed}

% For now, make this a no-op...
\newenvironment{codecell}{}

 \newenvironment{codeinput}{%
   \def\FrameCommand{\colorbox{inputbackground}}%
   \MakeFramed{\advance\hsize-\width \FrameRestore}}
 {\unskip\endMakeFramed}

\newenvironment{codeoutput}{%
   \def\FrameCommand{\colorbox{outputbackground}}%
   \vspace{-1.4em}
   \MakeFramed{\advance\hsize-\width \FrameRestore}}
 {\unskip\medskip\endMakeFramed}

\newenvironment{traceback}{%
   \def\FrameCommand{\colorbox{traceback}}%
   \MakeFramed{\advance\hsize-\width \FrameRestore}}
 {\endMakeFramed}

% Use and configure listings package for nicely formatted code
\usepackage{listingsutf8}
\lstset{
  language=python,
  inputencoding=utf8x,
  extendedchars=\true,
  aboveskip=\smallskipamount,
  belowskip=\smallskipamount,
  xleftmargin=2mm,
  breaklines=true,
  basicstyle=\small \ttfamily,
  showstringspaces=false,
  keywordstyle=\color{blue}\bfseries,
  commentstyle=\color{myteal},
  stringstyle=\color{darkgreen},
  identifierstyle=\color{darkorange},
  columns=fullflexible,  % tighter character kerning, like verb
}

% The hyperref package gives us a pdf with properly built
% internal navigation ('pdf bookmarks' for the table of contents,
% internal cross-reference links, web links for URLs, etc.)
\usepackage{hyperref}
\hypersetup{
  breaklinks=true,  % so long urls are correctly broken across lines
  colorlinks=true,
  urlcolor=blue,
  linkcolor=darkorange,
  citecolor=darkgreen,
  }

% hardcode size of all verbatim environments to be a bit smaller
\makeatletter 
\g@addto@macro\@verbatim\small\topsep=0.5em\partopsep=0pt
\makeatother 

% Prevent overflowing lines due to urls and other hard-to-break entities.
\sloppy

\begin{document}
\begin{center}
{\bf \Large MPI4PY Python}

{\bf \large Python in HPC}

{\bf TACC Training, Oct. 15, 2012}
\end{center}

\noindent Presenters:

\noindent {\bf Andy R. Terrel, PhD}\\
Texas Advanced Computing Center\\
University of Texas at Austin\\

\noindent {\bf Yaakoub El Khamra}\\
Texas Advanced Computing Center\\
University of Texas at Austin\\

\href{http://creativecommons.org/licenses/by/3.0/deed.en\_US}{\includegraphics{figures/creative_commons_logo.png}}\\

\noindent Python in HPC Tutorial by Terrel, and El Khamra is licensed
under a Creative Commons Attribution 3.0 Unported License. \\[2em]

\href{http://www.tacc.utexas.edu}{\includegraphics[scale=0.8]{figures/TACC_logo.png}} \qquad

\newpage

\newpage

\subsection{Interacting with the Tutorial Slides}

This tutorial is an interactive worksheet designed to encourage you to
try out the lessons during the demonstration. If you are looking at the
pdf version, we encourage you to download the updated version (see
previous slide) and try the interactive version.

To run the interactive version, you need a good Python environment
including:

\begin{itemize}
\item
  IPython version \textgreater{}= 13.0
\item
  Numpy version \textgreater{}= 1.5
\item
  Scipy
\item
  Matplotlib
\end{itemize}

Move to the directory containing the tarball and execute:

\begin{verbatim}
$ ipython notebook --pylab=inline
\end{verbatim}

We heartily endorse the
\href{https://store.continuum.io/cshop/anaconda}{Anaconda distribution}
and the \href{http://www.enthought.com/products/epd\_free.php}{Free
Enthought Python Distribution}.

\newpage

\subsection{Presentation mode}

 The slide show mode is only supported by an IPython development branch
version. To get it I recommend cloning from the official branch, adding
Matthias Carreau's remote, fetching and using his branch
slideshow\_extension2. Here are the commands:

\begin{verbatim}
git clone git://github.com/ipython/ipython.git # Official clone
cd ipython
git remote add carreau git://github.com/Carreau/ipython.git # Matthias' branch
git fetch carreau # Fetch the branches
git checkout carreau/slideshow_extension2 # Checkout the slideshow extension
python setup.py develop # Install the development version
ipython notebook # Check out the slideshows.
\end{verbatim}



\newpage

\subsection{Acknowledgements}

\begin{itemize}
\item
  Much of this tutorial adapts slide material from
  \href{http://www.cs.uiuc.edu/~wgropp/}{William Gropp}, University of
  Illinois
\item
  mpi4py examples developed by
  \href{http://www.bu.edu/pasi/people/lisandro-dalcin/}{Lisandro Dalcin}
\item
  \href{http://code.google.com/p/mpi4py/}{mpi4py} is a
  \href{http://www.cython.org/}{Cythonized} wrapper around
  \href{http://www.mpi-forum.org/}{MPI} originally developed by
  \href{http://plus.google.com/107621373684536061961/about}{Lisandro
  Dalcin}, CONICET
\end{itemize}

\newpage

\section{What is MPI}

\begin{itemize}
\item
  Message Passing Interface
\item
  Most useful on distributed memory machines
\item
  Many implementations, interfaces in C/C++/Fortran and Python
\item
  Why python?
\item
  Great for prototyping
\item
  Small to medium codes
\item
  Can I use it for production?
\item
  Yes, if the communication is not very frequent and rapid development
  is the primary concern
\end{itemize}

\newpage

\subsection{Why MPI?}

\begin{itemize}
\item
  \textbf{communicators} encapsulate communication spaces for library
  safety
\item
  \textbf{datatypes} reduce copying costs and permit heterogeneity
\item
  multiple \textbf{communication modes} allow more control of memory
  buffer management
\item
  extensive \textbf{collective operations} for scalable global
  communication
\item
  \textbf{process topologies} permit efficient process placement, user
  views of process layout
\item
  \textbf{profiling interface} encourages portable tools
\end{itemize}

It Scales!

\newpage

\subsection{MPI - Quick Review}

\begin{itemize}
\item
  processes can be collected into \textbf{groups}
\item
  each message is sent in a \textbf{context}, and must be received in
  the same context
\item
  a \textbf{communicator} encapsulates a context for a specific group
\item
  a given program may have many communicators with any level of overlap
\item
  two initial communicators
\item
  \texttt{MPI\_COMM\_WORLD} (all processes)
\item
  \texttt{MPI\_COMM\_SELF} (current process)
\end{itemize}

\newpage

\subsection{Communicators}

\begin{itemize}
\item
  processes can be collected into \textbf{groups}
\item
  each message is sent in a \textbf{context}, and must be received in
  the same context
\item
  a \textbf{communicator} encapsulates a context for a specific group
\item
  a given program may have many communicators with any level of overlap
\item
  two initial communicators
\item
  \texttt{MPI\_COMM\_WORLD} (all processes)
\item
  \texttt{MPI\_COMM\_SELF} (current process)
\end{itemize}

\newpage

\subsection{Datatypes}

\begin{itemize}
\item
  the data in a message to send or receive is described by address,
  count and datatype
\item
  a datatype is recursively defined as:
\item
  predefined, corresponding to a data type from the language (e.g.,
  \texttt{MPI\_INT}, \texttt{MPI\_DOUBLE})
\item
  a contiguous, strided block, or indexed array of blocks of MPI
  datatypes
\item
  an arbitrary structure of datatypes
\item
  there are MPI functions to construct custom datatypes
\end{itemize}

\newpage

\subsection{Tags}

\begin{itemize}
\item
  messages are sent with an accompanying user-defined integer tag to
  assist the receiving process in identifying the message
\item
  messages can be screened at the receiving end by specifying the
  expected tag, or not screened by using \texttt{MPI\_ANY\_TAG}
\end{itemize}

\newpage

\subsection{Functionality}

\begin{itemize}
\item
  There are hundreds of functions in the MPI standard, not all of them
  are necessarily available in MPI4Py, most commonly used are
\item
  No need to call MPI\_Init() or MPI\_Finalize()
\item
  MPI\_Init() is called when you import the module
\item
  MPI\_Finalize() is called before the Python process ends
\item
  To launch: mpirun --np \textless{} number of process \textgreater{}
  -machinefile \textless{} hostlist \textgreater{} python \textless{} my
  MPI4Py python script \textgreater{}
\end{itemize}

\subsection{Notes about Communicators}

\begin{itemize}
\item
  COMM\_WORLD is available (MPI.COMM\_WORLD)
\item
  To get size: MPI.COMM\_WORLD.Get\_size() or MPI.COMM\_WORLD.size
\item
  To get rank: MPI.COMM\_WORLD.Get\_rank() or MPI.COMM\_WORLD.rank
\item
  To get group (MPI Group): MPI.COMM\_WORLD.Get\_group() . This returns
  a Group object
\item
  Group objects can be used with Union(), Intersect(), Difference() to
  create new groups and new communicators using Create()
\item
  To duplicate a communicator: Clone() or Dup()
\item
  To split a communicator based on a color and key: Split()
\item
  Virtual topologies are supported!
\item
  Cartcomm, Graphcomm, Distgraphcomm fully supported
\item
  Use: Create\_cart(), Create\_graph()
\end{itemize}

\subsection{First Example: HelloWorld}

For interactive convenience, we load the parallel magic extensions and
make this view the active one for the automatic parallelism magics.

This is not necessary and in production codes likely won't be used, as
the engines will load their own MPI codes separately. But it makes it
easy to illustrate everything from within a single notebook here.

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
from IPython.parallel import Client
c = Client()
view = c[:]

%load_ext parallelmagic
view.activate()
\end{lstlisting}
\end{codeinput}
\end{codecell}
Use the autopx magic to make the rest of this cell execute on the
engines instead of locally

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
view.block = True
\end{lstlisting}
\end{codeinput}
\end{codecell}
\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
%autopx
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
%autopx enabled
\end{verbatim}
\end{codeoutput}
\end{codecell}
With autopx enabled, the next cell will actually execute \emph{entirely
on each engine}:

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
from mpi4py import MPI

size = MPI.COMM_WORLD.Get_size()
rank = MPI.COMM_WORLD.Get_rank()
name = MPI.Get_processor_name()

print("Helloworld! I am process %d of %d on %s.\n" % (rank, size, name))
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
[stdout:0] 
Helloworld! I am process 0 of 4 on orchid.local.

[stdout:1] 
Helloworld! I am process 3 of 4 on orchid.local.

[stdout:2] 
Helloworld! I am process 1 of 4 on orchid.local.

[stdout:3] 
Helloworld! I am process 2 of 4 on orchid.local.
\end{verbatim}
\end{codeoutput}
\end{codecell}
\newpage

\subsection{MPI Basics: (Blocking) Send}

\begin{verbatim}
   int MPI_Send(void* buf, int count, MPI_Datatype type, 
   int dest, int tag, MPI_Comm comm)
\end{verbatim}

Python (mpi4py)

\texttt{Comm.Send(self, buf, int dest=0, int tag=0)   Comm.send(self, obj=None, int dest=0, int tag=0)}

\subsection{MPI Basics: (Blocking) Recv}

\begin{verbatim}
   int MPI_Recv(void* buf, int count, MPI_Datatype type, 
   int source, int tag, MPI_Comm comm, MPI_Status status)
\end{verbatim}

Python (mpi4py)

\texttt{comm.Recv(self, buf, int source=0, int tag=0,    Status status=None)   comm.recv(self, obj=None, int source=0,    int tag=0, Status status=None)}

\subsection{MPI Basics: Synchronization}

\begin{verbatim}
   int MPI_Barrier(MPI_Comm comm)
\end{verbatim}

Python (mpi4py)

\texttt{comm.Barrier(self)    comm.barrier(self)}

\newpage

\subsection{Timing and Profiling}

the elapsed (wall-clock) time between two points in an MPI program can
be computed using MPI\_Wtime:

\texttt{t1 = MPI.Wtime()       t2 = MPI.Wtime()       print("time elapsed is: \%e\textbackslash{}n" \% (t2-t1))}

\subsection{Send/Receive Example (lowercase convenience methods)}


\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

if rank == 0:
   data = {'a': 7, 'b': 3.14}
   comm.send(data, dest=1, tag=11)
elif rank == 1:
   data = comm.recv(source=0, tag=11)
   print data
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
[stdout:2] {'a': 7, 'b': 3.14}
\end{verbatim}
\end{codeoutput}
\end{codecell}

\subsection{Send/Receive Example (MPI API on numpy)}

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
from mpi4py import MPI
import numpy

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

# pass explicit MPI datatypes
if rank == 0:
   data = numpy.arange(1000, dtype='i')
   comm.Send([data, MPI.INT], dest=1, tag=77)
elif rank == 1:
   data = numpy.empty(1000, dtype='i')
   comm.Recv([data, MPI.INT], source=0, tag=77)

# or take advantage of automatic MPI datatype discovery
if rank == 0:
   data = numpy.arange(100, dtype=numpy.float64)
   comm.Send(data, dest=1, tag=13)
elif rank == 1:
   data = numpy.empty(100, dtype=numpy.float64)
   comm.Recv(data, source=0, tag=13)
   print data
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
[stdout:2] 
[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.
  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.  29.
  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.  43.  44.
  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.  57.  58.  59.
  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.  71.  72.  73.  74.
  75.  76.  77.  78.  79.  80.  81.  82.  83.  84.  85.  86.  87.  88.  89.
  90.  91.  92.  93.  94.  95.  96.  97.  98.  99.]
\end{verbatim}
\end{codeoutput}
\end{codecell}

\subsection{Broadcast/Scatter/Gather}

\begin{figure}[htbp]
\centering
\includegraphics{figures/broadcast_scatter_gather.png}
\caption{broadcast\_scatter\_gather}
\end{figure}

\subsection{Broadcast Example}

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()

if rank == 0:
   data = {'key1' : [7, 2.72, 2+3j],
           'key2' : ( 'abc', 'xyz')}
else:
   data = None
data = comm.bcast(data, root=0)
print "bcast finished and data \
 on rank %d is: "%comm.rank, data

\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
[stdout:0] bcast finished and data  on rank 0 is:  {'key2': ('abc', 'xyz'), 'key1': [7, 2.72, (2+3j)]}
[stdout:1] bcast finished and data  on rank 3 is:  {'key2': ('abc', 'xyz'), 'key1': [7, 2.72, (2+3j)]}
[stdout:2] bcast finished and data  on rank 1 is:  {'key2': ('abc', 'xyz'), 'key1': [7, 2.72, (2+3j)]}
[stdout:3] bcast finished and data  on rank 2 is:  {'key2': ('abc', 'xyz'), 'key1': [7, 2.72, (2+3j)]}
\end{verbatim}
\end{codeoutput}
\end{codecell}
\subsection{Scatter Example:}

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
from mpi4py import MPI

comm = MPI.COMM_WORLD
size = comm.Get_size()
rank = comm.Get_rank()

if rank == 0:
   data = [(i+1)**2 for i in range(size)]
else:
   data = None
data = comm.scatter(data, root=0)
assert data == (rank+1)**2
print "data on rank %d is: "%comm.rank, data
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
[stdout:0] data on rank 0 is:  1
[stdout:1] data on rank 3 is:  16
[stdout:2] data on rank 1 is:  4
[stdout:3] data on rank 2 is:  9
\end{verbatim}
\end{codeoutput}
\end{codecell}

\subsection{Gather (and Barrier) Example:}

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
from mpi4py import MPI

comm = MPI.COMM_WORLD
size = comm.Get_size()
rank = comm.Get_rank()

data = (rank+1)**2
print "before gather, data on \
  rank %d is: "%rank, data

comm.Barrier()
data = comm.gather(data, root=0)
if rank == 0:
   for i in range(size):
       assert data[i] == (i+1)**2
else:
   assert data is None
print "data on rank: %d is: "%rank, data
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
[stdout:0] 
before gather, data on   rank 0 is:  1
data on rank: 0 is:  [1, 4, 9, 16]
[stdout:1] 
before gather, data on   rank 3 is:  16
data on rank: 3 is:  None
[stdout:2] 
before gather, data on   rank 1 is:  4
data on rank: 1 is:  None
[stdout:3] 
before gather, data on   rank 2 is:  9
data on rank: 2 is:  None
\end{verbatim}
\end{codeoutput}
\end{codecell}

\subsection{Reduce and Scan}

\begin{figure}[htbp]
\centering
\includegraphics{figures/reduce_scan.png}
\caption{reduce\_scan}
\end{figure}

\subsection{Reduce Example:}

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
from mpi4py import MPI
comm = MPI.COMM_WORLD

sendmsg = comm.rank

recvmsg1 = comm.reduce(sendmsg, op=MPI.SUM, root=0)

recvmsg2 = comm.allreduce(sendmsg)
print recvmsg2
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
[stdout:0] 6
[stdout:1] 6
[stdout:2] 6
[stdout:3] 6
\end{verbatim}
\end{codeoutput}
\end{codecell}

\subsection{Compute Pi Example}


\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
from mpi4py import MPI
import math

def compute_pi(n, start=0, step=1):
    h = 1.0 / n
    s = 0.0
    for i in range(start, n, step):
        x = h * (i + 0.5)
        s += 4.0 / (1.0 + x**2)
    return s * h

comm = MPI.COMM_WORLD
nprocs = comm.Get_size()
myrank = comm.Get_rank()
if myrank == 0:
    n = 10
else:
    n = None

n = comm.bcast(n, root=0)

mypi = compute_pi(n, myrank, nprocs)

pi = comm.reduce(mypi, op=MPI.SUM, root=0)

if myrank == 0:
    error = abs(pi - math.pi)
    print ("pi is approximately %.16f, error is %.16f" % (pi, error))
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
[stdout:0] pi is approximately 3.1424259850010983, error is 0.0008333314113051
\end{verbatim}
\end{codeoutput}
\end{codecell}

\subsection{Mandelbrot Set Example}


\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
def mandelbrot (x, y, maxit):
    c = x + y*1j
    z = 0 + 0j
    it = 0
    while abs(z) < 2 and it < maxit:
        z = z**2 + c
        it += 1
    return it

x1, x2 = -2.0, 1.0
y1, y2 = -1.0, 1.0
w, h = 150, 100
maxit = 127

from mpi4py import MPI
import numpy
  
comm = MPI.COMM_WORLD
size = comm.Get_size()
rank = comm.Get_rank()

# number of rows to compute here
N = h // size + (h % size > rank)

# first row to compute here
start = comm.scan(N)-N

# array to store local result
Cl = numpy.zeros([N, w], dtype='i')

# compute owned rows

dx = (x2 - x1) / w
dy = (y2 - y1) / h

for i in range(N):
    y = y1 + (i + start) * dy
    for j in range(w):
        x = x1 + j * dx
        Cl[i, j] = mandelbrot(x, y, maxit)

# gather results at root (process 0)
counts = comm.gather(N, root=0)
C = None
if rank == 0:
    C = numpy.zeros([h, w], dtype='i')

rowtype = MPI.INT.Create_contiguous(w)
rowtype.Commit()

comm.Gatherv(sendbuf=[Cl, MPI.INT], recvbuf=[C, (counts, None), rowtype],root=0)

rowtype.Free()
\end{lstlisting}
\end{codeinput}
\end{codecell}
We now need to ``pull'' the C array for plotting so we disable autopx.
Make sure to re-enable it later on

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
%autopx
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
%autopx disabled
\end{verbatim}
\end{codeoutput}
\end{codecell}
\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
# CC is an array of C from all ranks, so we use CC[0]
CC = view['C']
ranks = view['rank']

# Do the plotting
from matplotlib import pyplot
# Some magic to get to MPI4PY rank 0, not necessarily engine id 0
pyplot.imshow(CC[ranks.index(0)], aspect='equal')
pyplot.spectral()
pyplot.show()
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{center}
\includegraphics[width=6in]{06_MPI4Py_Python_files/06_MPI4Py_Python_fig_00.png}
\par
\end{center}
\end{codeoutput}
\end{codecell}
Toggle autopx back

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
%autopx
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
%autopx enabled
\end{verbatim}
\end{codeoutput}
\end{codecell}

\subsection{Advanced Capabilities}

\begin{itemize}
\item
  MPI4Py supports dynamic processes through spawning: Spawn(), Connect()
  and Disconnect()
\item
  MPI4PY supports one sided communication Put(), Get(), Accumulate()
\item
  MPI4Py supports MPI-IO: Open(), Close(), Get\_view() and Set\_view()
\end{itemize}

\subsection{More Comprehensive mpi4py Tutorials}
* basics - http://mpi4py.scipy.org/docs/usrman/tutorial.html * advanced
- http://www.bu.edu/pasi2011/01/Lisandro-Dalcin-mpi4py.pdf Timing and
Profiling

\subsection{Interesting Scalable Applications and Tools}

\begin{itemize}
\item
  PyTrilinos - http://trilinos.sandia.gov/packages/pytrilinos/
\item
  petsc4py - http://code.google.com/p/petsc4py/
\item
  PyClaw - http://numerics.kaust.edu.sa/pyclaw/
\item
  GPAW - https://wiki.fysik.dtu.dk/gpaw/
\end{itemize}

\subsection{--\textgreater{} See Appendix 01 References for more
resources}

\end{document}
